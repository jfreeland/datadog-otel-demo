# Copyright The OpenTelemetry Authors
# SPDX-License-Identifier: Apache-2.0
# This file is generated by 'make generate-kubernetes-manifests'
---
apiVersion: v1
kind: Namespace
metadata:
  name: otel-demo
---
# Source: opentelemetry-demo/charts/opensearch/templates/poddisruptionbudget.yaml
#apiVersion: policy/v1
#kind: PodDisruptionBudget
#metadata:
#  name: "otel-demo-opensearch-pdb"
#  labels:
#    app.kubernetes.io/name: opensearch
#    app.kubernetes.io/instance: opentelemetry-demo
#    app.kubernetes.io/version: "2.18.0"
#    app.kubernetes.io/component: otel-demo-opensearch
#spec:
#  maxUnavailable: 1
#  selector:
#    matchLabels:
#      app.kubernetes.io/name: opensearch
#      app.kubernetes.io/instance: opentelemetry-demo
#---
# Source: opentelemetry-demo/charts/grafana/templates/serviceaccount.yaml
#apiVersion: v1
#kind: ServiceAccount
#automountServiceAccountToken: false
#metadata:
#  labels:
#    app.kubernetes.io/name: grafana
#    app.kubernetes.io/instance: opentelemetry-demo
#    app.kubernetes.io/version: "11.3.0"
#  name: opentelemetry-demo-grafana
#  namespace: otel-demo
#---
# Source: opentelemetry-demo/charts/jaeger/templates/allinone-sa.yaml
#apiVersion: v1
#kind: ServiceAccount
#metadata:
#  name: opentelemetry-demo-jaeger
#  labels:
#    app.kubernetes.io/name: jaeger
#    app.kubernetes.io/instance: opentelemetry-demo
#    app.kubernetes.io/version: "1.53.0"
#    app.kubernetes.io/component: all-in-one
#automountServiceAccountToken: true
#---
# Source: opentelemetry-demo/charts/opentelemetry-collector/templates/serviceaccount.yaml
#apiVersion: v1
#kind: ServiceAccount
#metadata:
#  name: opentelemetry-demo-otelcol
#  namespace: otel-demo
#  labels:
#    app.kubernetes.io/name: otelcol
#    app.kubernetes.io/instance: opentelemetry-demo
#    app.kubernetes.io/version: "0.113.0"
#---
# Source: opentelemetry-demo/charts/prometheus/templates/serviceaccount.yaml
#apiVersion: v1
#kind: ServiceAccount
#metadata:
#  labels:
#    app.kubernetes.io/component: server
#    app.kubernetes.io/name: prometheus
#    app.kubernetes.io/instance: opentelemetry-demo
#    app.kubernetes.io/version: v2.55.1
#    app.kubernetes.io/part-of: prometheus
#  name: opentelemetry-demo-prometheus-server
#  namespace: otel-demo
#  annotations:
#    {}
#---
# Source: opentelemetry-demo/templates/serviceaccount.yaml
apiVersion: v1
kind: ServiceAccount
metadata:
  name: opentelemetry-demo
  labels:
    opentelemetry.io/name: opentelemetry-demo
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/name: opentelemetry-demo
    app.kubernetes.io/version: "1.12.0"
    app.kubernetes.io/part-of: opentelemetry-demo
---
# Source: opentelemetry-demo/charts/grafana/templates/secret.yaml
#apiVersion: v1
#kind: Secret
#metadata:
#  name: opentelemetry-demo-grafana
#  namespace: otel-demo
#  labels:
#    app.kubernetes.io/name: grafana
#    app.kubernetes.io/instance: opentelemetry-demo
#    app.kubernetes.io/version: "11.3.0"
#type: Opaque
#data:
#  admin-user: "YWRtaW4="
#  admin-password: "YWRtaW4="
#  ldap-toml: ""
#---
# Source: opentelemetry-demo/charts/grafana/templates/configmap.yaml
#apiVersion: v1
#kind: ConfigMap
#metadata:
#  name: opentelemetry-demo-grafana
#  namespace: otel-demo
#  labels:
#    app.kubernetes.io/name: grafana
#    app.kubernetes.io/instance: opentelemetry-demo
#    app.kubernetes.io/version: "11.3.0"
#data:
#  plugins: grafana-opensearch-datasource
#  grafana.ini: |
#    [analytics]
#    check_for_updates = true
#    [auth]
#    disable_login_form = true
#    [auth.anonymous]
#    enabled = true
#    org_name = Main Org.
#    org_role = Admin
#    [grafana_net]
#    url = https://grafana.net
#    [log]
#    mode = console
#    [paths]
#    data = /var/lib/grafana/
#    logs = /var/log/grafana
#    plugins = /var/lib/grafana/plugins
#    provisioning = /etc/grafana/provisioning
#    [server]
#    domain = ''
#    root_url = %(protocol)s://%(domain)s:%(http_port)s/grafana
#    serve_from_sub_path = true
#  datasources.yaml: |
#    apiVersion: 1
#    datasources:
#    - editable: true
#      isDefault: true
#      jsonData:
#        exemplarTraceIdDestinations:
#        - datasourceUid: webstore-traces
#          name: trace_id
#        - name: trace_id
#          url: http://localhost:8080/jaeger/ui/trace/$${__value.raw}
#          urlDisplayLabel: View in Jaeger UI
#      name: Prometheus
#      type: prometheus
#      uid: webstore-metrics
#      url: http://opentelemetry-demo-prometheus-server:9090
#    - editable: true
#      isDefault: false
#      name: Jaeger
#      type: jaeger
#      uid: webstore-traces
#      url: http://opentelemetry-demo-jaeger-query:16686/jaeger/ui
#    - access: proxy
#      editable: true
#      isDefault: false
#      jsonData:
#        database: otel
#        flavor: opensearch
#        logLevelField: severity
#        logMessageField: body
#        pplEnabled: true
#        timeField: observedTimestamp
#        version: 2.18.0
#      name: OpenSearch
#      type: grafana-opensearch-datasource
#      url: http://otel-demo-opensearch:9200/
#  dashboardproviders.yaml: |
#    apiVersion: 1
#    providers:
#    - disableDeletion: false
#      editable: true
#      folder: ""
#      name: default
#      options:
#        path: /var/lib/grafana/dashboards/default
#      orgId: 1
#      type: file
#---
# Source: opentelemetry-demo/charts/opensearch/templates/configmap.yaml
#apiVersion: v1
#kind: ConfigMap
#metadata:
#  name: otel-demo-opensearch-config
#  labels:
#    app.kubernetes.io/name: opensearch
#    app.kubernetes.io/instance: opentelemetry-demo
#    app.kubernetes.io/version: "2.18.0"
#    app.kubernetes.io/component: otel-demo-opensearch
#data:
#  opensearch.yml: |
#    cluster.name: opensearch-cluster
#
#    # Bind to all interfaces because we don't know what IP address Docker will assign to us.
#    network.host: 0.0.0.0
#
#    # Setting network.host to a non-loopback address enables the annoying bootstrap checks. "Single-node" mode disables them again.
#    # Implicitly done if ".singleNode" is set to "true".
#    # discovery.type: single-node
#
#    # Start OpenSearch Security Demo Configuration
#    # WARNING: revise all the lines below before you go into production
#    plugins:
#      security:
#        ssl:
#          transport:
#            pemcert_filepath: esnode.pem
#            pemkey_filepath: esnode-key.pem
#            pemtrustedcas_filepath: root-ca.pem
#            enforce_hostname_verification: false
#          http:
#            enabled: true
#            pemcert_filepath: esnode.pem
#            pemkey_filepath: esnode-key.pem
#            pemtrustedcas_filepath: root-ca.pem
#        allow_unsafe_democertificates: true
#        allow_default_init_securityindex: true
#        authcz:
#          admin_dn:
#            - CN=kirk,OU=client,O=client,L=test,C=de
#        audit.type: internal_opensearch
#        enable_snapshot_restore_privilege: true
#        check_snapshot_restore_write_privileges: true
#        restapi:
#          roles_enabled: ["all_access", "security_rest_api_access"]
#        system_indices:
#          enabled: true
#          indices:
#            [
#              ".opendistro-alerting-config",
#              ".opendistro-alerting-alert*",
#              ".opendistro-anomaly-results*",
#              ".opendistro-anomaly-detector*",
#              ".opendistro-anomaly-checkpoints",
#              ".opendistro-anomaly-detection-state",
#              ".opendistro-reports-*",
#              ".opendistro-notifications-*",
#              ".opendistro-notebooks",
#              ".opendistro-asynchronous-search-response*",
#            ]
#    ######## End OpenSearch Security Demo Configuration ########
#---
# Source: opentelemetry-demo/charts/opentelemetry-collector/templates/configmap.yaml
#apiVersion: v1
#kind: ConfigMap
#metadata:
#  name: opentelemetry-demo-otelcol
#  namespace: otel-demo
#  labels:
#    app.kubernetes.io/name: otelcol
#    app.kubernetes.io/instance: opentelemetry-demo
#    app.kubernetes.io/version: "0.113.0"
#
#data:
#  relay: |
#    connectors:
#      spanmetrics: {}
#    exporters:
#      debug: {}
#      opensearch:
#        http:
#          endpoint: http://otel-demo-opensearch:9200
#          tls:
#            insecure: true
#        logs_index: otel
#      otlp:
#        endpoint: 'opentelemetry-demo-jaeger-collector:4317'
#        tls:
#          insecure: true
#      otlphttp/prometheus:
#        endpoint: http://opentelemetry-demo-prometheus-server:9090/api/v1/otlp
#        tls:
#          insecure: true
#    extensions:
#      health_check:
#        endpoint: ${env:MY_POD_IP}:13133
#    processors:
#      batch: {}
#      k8sattributes:
#        extract:
#          metadata:
#          - k8s.namespace.name
#          - k8s.deployment.name
#          - k8s.statefulset.name
#          - k8s.daemonset.name
#          - k8s.cronjob.name
#          - k8s.job.name
#          - k8s.node.name
#          - k8s.pod.name
#          - k8s.pod.uid
#          - k8s.pod.start_time
#        passthrough: false
#        pod_association:
#        - sources:
#          - from: resource_attribute
#            name: k8s.pod.ip
#        - sources:
#          - from: resource_attribute
#            name: k8s.pod.uid
#        - sources:
#          - from: connection
#      memory_limiter:
#        check_interval: 5s
#        limit_percentage: 80
#        spike_limit_percentage: 25
#      resource:
#        attributes:
#        - action: insert
#          from_attribute: k8s.pod.uid
#          key: service.instance.id
#      transform:
#        error_mode: ignore
#        trace_statements:
#        - context: span
#          statements:
#          - replace_pattern(name, "\\?.*", "")
#          - replace_match(name, "GET /api/products/*", "GET /api/products/{productId}")
#    receivers:
#      httpcheck/frontendproxy:
#        targets:
#        - endpoint: http://opentelemetry-demo-frontendproxy:8080
#      jaeger:
#        protocols:
#          grpc:
#            endpoint: ${env:MY_POD_IP}:14250
#          thrift_compact:
#            endpoint: ${env:MY_POD_IP}:6831
#          thrift_http:
#            endpoint: ${env:MY_POD_IP}:14268
#      otlp:
#        protocols:
#          grpc:
#            endpoint: ${env:MY_POD_IP}:4317
#          http:
#            cors:
#              allowed_origins:
#              - http://*
#              - https://*
#            endpoint: ${env:MY_POD_IP}:4318
#      prometheus:
#        config:
#          scrape_configs:
#          - job_name: opentelemetry-collector
#            scrape_interval: 10s
#            static_configs:
#            - targets:
#              - ${env:MY_POD_IP}:8888
#      redis:
#        collection_interval: 10s
#        endpoint: valkey-cart:6379
#      zipkin:
#        endpoint: ${env:MY_POD_IP}:9411
#    service:
#      extensions:
#      - health_check
#      pipelines:
#        logs:
#          exporters:
#          - opensearch
#          - debug
#          processors:
#          - k8sattributes
#          - memory_limiter
#          - resource
#          - batch
#          receivers:
#          - otlp
#        metrics:
#          exporters:
#          - otlphttp/prometheus
#          - debug
#          processors:
#          - k8sattributes
#          - memory_limiter
#          - resource
#          - batch
#          receivers:
#          - httpcheck/frontendproxy
#          - redis
#          - otlp
#          - spanmetrics
#        traces:
#          exporters:
#          - otlp
#          - debug
#          - spanmetrics
#          processors:
#          - k8sattributes
#          - memory_limiter
#          - resource
#          - transform
#          - batch
#          receivers:
#          - otlp
#          - jaeger
#          - zipkin
#      telemetry:
#        metrics:
#          address: ${env:MY_POD_IP}:8888
#---
# Source: opentelemetry-demo/charts/prometheus/templates/cm.yaml
#apiVersion: v1
#kind: ConfigMap
#metadata:
#  labels:
#    app.kubernetes.io/component: server
#    app.kubernetes.io/name: prometheus
#    app.kubernetes.io/instance: opentelemetry-demo
#    app.kubernetes.io/version: v2.55.1
#    app.kubernetes.io/part-of: prometheus
#  name: opentelemetry-demo-prometheus-server
#  namespace: otel-demo
#data:
#  allow-snippet-annotations: "false"
#  alerting_rules.yml: |
#    {}
#  alerts: |
#    {}
#  prometheus.yml: |
#    global:
#      evaluation_interval: 30s
#      scrape_interval: 5s
#      scrape_timeout: 3s
#    storage:
#      tsdb:
#        out_of_order_time_window: 30m
#    rule_files:
#    - /etc/config/recording_rules.yml
#    - /etc/config/alerting_rules.yml
#    - /etc/config/rules
#    - /etc/config/alerts
#    scrape_configs:
#    - honor_labels: true
#      job_name: otel-collector
#      kubernetes_sd_configs:
#      - namespaces:
#          own_namespace: true
#        role: pod
#      relabel_configs:
#      - action: keep
#        regex: true
#        source_labels:
#        - __meta_kubernetes_pod_annotation_opentelemetry_community_demo
#  recording_rules.yml: |
#    {}
#  rules: |
#    {}
#---
# Source: opentelemetry-demo/templates/flagd-config.yaml
apiVersion: v1
kind: ConfigMap
metadata:
  name: opentelemetry-demo-flagd-config
  namespace: otel-demo
  labels:
    opentelemetry.io/name: opentelemetry-demo
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/name: opentelemetry-demo
    app.kubernetes.io/version: "1.12.0"
    app.kubernetes.io/part-of: opentelemetry-demo
data:
  demo.flagd.json: |
    {
      "$schema": "https://flagd.dev/schema/v0/flags.json",
      "flags": {
        "productCatalogFailure": {
          "description": "Fail product catalog service on a specific product",
          "state": "ENABLED",
          "variants": {
            "on": true,
            "off": false
          },
          "defaultVariant": "off"
        },
        "recommendationServiceCacheFailure": {
          "description": "Fail recommendation service cache",
          "state": "ENABLED",
          "variants": {
            "on": true,
            "off": false
          },
          "defaultVariant": "off"
        },
        "adServiceManualGc": {
          "description": "Triggers full manual garbage collections in the ad service",
          "state": "ENABLED",
          "variants": {
            "on": true,
            "off": false
          },
          "defaultVariant": "off"
        },
        "adServiceHighCpu": {
          "description": "Triggers high cpu load in the ad service",
          "state": "ENABLED",
          "variants": {
            "on": true,
            "off": false
          },
          "defaultVariant": "off"
        },
        "adServiceFailure": {
          "description": "Fail ad service",
          "state": "ENABLED",
          "variants": {
            "on": true,
            "off": false
          },
          "defaultVariant": "off"
        },
        "kafkaQueueProblems": {
          "description": "Overloads Kafka queue while simultaneously introducing a consumer side delay leading to a lag spike",
          "state": "ENABLED",
          "variants": {
            "on": 100,
            "off": 0
          },
          "defaultVariant": "off"
        },
        "cartServiceFailure": {
          "description": "Fail cart service",
          "state": "ENABLED",
          "variants": {
            "on": true,
            "off": false
          },
          "defaultVariant": "off"
        },
        "paymentServiceFailure": {
          "description": "Fail payment service charge requests",
          "state": "ENABLED",
          "variants": {
            "on": true,
            "off": false
          },
          "defaultVariant": "off"
        },
        "paymentServiceUnreachable": {
          "description": "Payment service is unavailable",
          "state": "ENABLED",
          "variants": {
            "on": true,
            "off": false
          },
          "defaultVariant": "off"
        },
        "loadgeneratorFloodHomepage": {
          "description": "Flood the frontend with a large amount of requests.",
          "state": "ENABLED",
          "variants": {
            "on": 100,
            "off": 0
          },
          "defaultVariant": "off"
        },
        "imageSlowLoad": {
          "description": "slow loading images in the frontend",
          "state": "ENABLED",
          "variants": {
            "10sec": 10000,
            "5sec": 5000,
            "off": 0
          },
          "defaultVariant": "off"
        }
      }
    }
---
# Source: opentelemetry-demo/templates/grafana-dashboards.yaml
# Source: opentelemetry-demo/charts/grafana/templates/clusterrole.yaml
#kind: ClusterRole
#apiVersion: rbac.authorization.k8s.io/v1
#metadata:
#  labels:
#    app.kubernetes.io/name: grafana
#    app.kubernetes.io/instance: opentelemetry-demo
#    app.kubernetes.io/version: "11.3.0"
#  name: opentelemetry-demo-grafana-clusterrole
#rules: []
#---
# Source: opentelemetry-demo/charts/opentelemetry-collector/templates/clusterrole.yaml
#apiVersion: rbac.authorization.k8s.io/v1
#kind: ClusterRole
#metadata:
#  name: opentelemetry-demo-otelcol
#  labels:
#    app.kubernetes.io/name: otelcol
#    app.kubernetes.io/instance: opentelemetry-demo
#    app.kubernetes.io/version: "0.113.0"
#
#rules:
#  - apiGroups: [""]
#    resources: ["pods", "namespaces"]
#    verbs: ["get", "watch", "list"]
#  - apiGroups: ["apps"]
#    resources: ["replicasets"]
#    verbs: ["get", "list", "watch"]
#  - apiGroups: ["extensions"]
#    resources: ["replicasets"]
#    verbs: ["get", "list", "watch"]
#---
# Source: opentelemetry-demo/charts/prometheus/templates/clusterrole.yaml
#apiVersion: rbac.authorization.k8s.io/v1
#kind: ClusterRole
#metadata:
#  labels:
#    app.kubernetes.io/component: server
#    app.kubernetes.io/name: prometheus
#    app.kubernetes.io/instance: opentelemetry-demo
#    app.kubernetes.io/version: v2.55.1
#    app.kubernetes.io/part-of: prometheus
#  name: opentelemetry-demo-prometheus-server
#rules:
#  - apiGroups:
#      - ""
#    resources:
#      - nodes
#      - nodes/proxy
#      - nodes/metrics
#      - services
#      - endpoints
#      - pods
#      - ingresses
#      - configmaps
#    verbs:
#      - get
#      - list
#      - watch
#  - apiGroups:
#      - "extensions"
#      - "networking.k8s.io"
#    resources:
#      - ingresses/status
#      - ingresses
#    verbs:
#      - get
#      - list
#      - watch
#  - apiGroups:
#      - "discovery.k8s.io"
#    resources:
#      - endpointslices
#    verbs:
#      - get
#      - list
#      - watch
#  - nonResourceURLs:
#      - "/metrics"
#    verbs:
#      - get
#---
# Source: opentelemetry-demo/charts/grafana/templates/clusterrolebinding.yaml
#kind: ClusterRoleBinding
#apiVersion: rbac.authorization.k8s.io/v1
#metadata:
#  name: opentelemetry-demo-grafana-clusterrolebinding
#  labels:
#    app.kubernetes.io/name: grafana
#    app.kubernetes.io/instance: opentelemetry-demo
#    app.kubernetes.io/version: "11.3.0"
#subjects:
#  - kind: ServiceAccount
#    name: opentelemetry-demo-grafana
#    namespace: otel-demo
#roleRef:
#  kind: ClusterRole
#  name: opentelemetry-demo-grafana-clusterrole
#  apiGroup: rbac.authorization.k8s.io
#---
# Source: opentelemetry-demo/charts/opentelemetry-collector/templates/clusterrolebinding.yaml
#apiVersion: rbac.authorization.k8s.io/v1
#kind: ClusterRoleBinding
#metadata:
#  name: opentelemetry-demo-otelcol
#  labels:
#    app.kubernetes.io/name: otelcol
#    app.kubernetes.io/instance: opentelemetry-demo
#    app.kubernetes.io/version: "0.113.0"
#
#roleRef:
#  apiGroup: rbac.authorization.k8s.io
#  kind: ClusterRole
#  name: opentelemetry-demo-otelcol
#subjects:
#  - kind: ServiceAccount
#    name: opentelemetry-demo-otelcol
#    namespace: otel-demo
#---
# Source: opentelemetry-demo/charts/prometheus/templates/clusterrolebinding.yaml
#apiVersion: rbac.authorization.k8s.io/v1
#kind: ClusterRoleBinding
#metadata:
#  labels:
#    app.kubernetes.io/component: server
#    app.kubernetes.io/name: prometheus
#    app.kubernetes.io/instance: opentelemetry-demo
#    app.kubernetes.io/version: v2.55.1
#    app.kubernetes.io/part-of: prometheus
#  name: opentelemetry-demo-prometheus-server
#subjects:
#  - kind: ServiceAccount
#    name: opentelemetry-demo-prometheus-server
#    namespace: otel-demo
#roleRef:
#  apiGroup: rbac.authorization.k8s.io
#  kind: ClusterRole
#  name: opentelemetry-demo-prometheus-server
#---
# Source: opentelemetry-demo/charts/grafana/templates/role.yaml
#apiVersion: rbac.authorization.k8s.io/v1
#kind: Role
#metadata:
#  name: opentelemetry-demo-grafana
#  namespace: otel-demo
#  labels:
#    app.kubernetes.io/name: grafana
#    app.kubernetes.io/instance: opentelemetry-demo
#    app.kubernetes.io/version: "11.3.0"
#rules: []
#---
# Source: opentelemetry-demo/charts/grafana/templates/rolebinding.yaml
#apiVersion: rbac.authorization.k8s.io/v1
#kind: RoleBinding
#metadata:
#  name: opentelemetry-demo-grafana
#  namespace: otel-demo
#  labels:
#    app.kubernetes.io/name: grafana
#    app.kubernetes.io/instance: opentelemetry-demo
#    app.kubernetes.io/version: "11.3.0"
#roleRef:
#  apiGroup: rbac.authorization.k8s.io
#  kind: Role
#  name: opentelemetry-demo-grafana
#subjects:
#  - kind: ServiceAccount
#    name: opentelemetry-demo-grafana
#    namespace: otel-demo
#---
# Source: opentelemetry-demo/charts/grafana/templates/service.yaml
#apiVersion: v1
#kind: Service
#metadata:
#  name: opentelemetry-demo-grafana
#  namespace: otel-demo
#  labels:
#    app.kubernetes.io/name: grafana
#    app.kubernetes.io/instance: opentelemetry-demo
#    app.kubernetes.io/version: "11.3.0"
#spec:
#  type: ClusterIP
#  ports:
#    - name: service
#      port: 80
#      protocol: TCP
#      targetPort: 3000
#  selector:
#    app.kubernetes.io/name: grafana
#    app.kubernetes.io/instance: opentelemetry-demo
#---
# Source: opentelemetry-demo/charts/jaeger/templates/allinone-agent-svc.yaml
#apiVersion: v1
#kind: Service
#metadata:
#  name: opentelemetry-demo-jaeger-agent
#  labels:
#    app.kubernetes.io/name: jaeger
#    app.kubernetes.io/instance: opentelemetry-demo
#    app.kubernetes.io/version: "1.53.0"
#    app.kubernetes.io/component: service-agent
#spec:
#  clusterIP: None
#  ports:
#    - name: zk-compact-trft
#      port: 5775
#      protocol: UDP
#      targetPort: 0
#    - name: config-rest
#      port: 5778
#      targetPort: 0
#    - name: jg-compact-trft
#      port: 6831
#      protocol: UDP
#      targetPort: 0
#    - name: jg-binary-trft
#      port: 6832
#      protocol: UDP
#      targetPort: 0
#  selector:
#    app.kubernetes.io/name: jaeger
#    app.kubernetes.io/instance: opentelemetry-demo
#    app.kubernetes.io/component: all-in-one
#---
# Source: opentelemetry-demo/charts/jaeger/templates/allinone-collector-svc.yaml
#apiVersion: v1
#kind: Service
#metadata:
#  name: opentelemetry-demo-jaeger-collector
#  labels:
#    app.kubernetes.io/name: jaeger
#    app.kubernetes.io/instance: opentelemetry-demo
#    app.kubernetes.io/version: "1.53.0"
#    app.kubernetes.io/component: service-collector
#spec:
#  clusterIP: None
#  ports:
#    - name: http-zipkin
#      port: 9411
#      targetPort: 0
#      appProtocol: http
#    - name: grpc-http
#      port: 14250
#      targetPort: 0
#      appProtocol: grpc
#    - name: c-tchan-trft
#      port: 14267
#      targetPort: 0
#    - name: http-c-binary-trft
#      port: 14268
#      targetPort: 0
#      appProtocol: http
#    - name: otlp-grpc
#      port: 4317
#      targetPort: 0
#      appProtocol: grpc
#    - name: otlp-http
#      port: 4318
#      targetPort: 0
#      appProtocol: http
#  selector:
#    app.kubernetes.io/name: jaeger
#    app.kubernetes.io/instance: opentelemetry-demo
#    app.kubernetes.io/component: all-in-one
#---
# Source: opentelemetry-demo/charts/jaeger/templates/allinone-query-svc.yaml
#apiVersion: v1
#kind: Service
#metadata:
#  name: opentelemetry-demo-jaeger-query
#  labels:
#    app.kubernetes.io/name: jaeger
#    app.kubernetes.io/instance: opentelemetry-demo
#    app.kubernetes.io/version: "1.53.0"
#    app.kubernetes.io/component: service-query
#spec:
#  clusterIP: None
#  ports:
#    - name: http-query
#      port: 16686
#      targetPort: 16686
#    - name: grpc-query
#      port: 16685
#      targetPort: 16685
#  selector:
#    app.kubernetes.io/name: jaeger
#    app.kubernetes.io/instance: opentelemetry-demo
#    app.kubernetes.io/component: all-in-one
#---
# Source: opentelemetry-demo/charts/opensearch/templates/service.yaml
#kind: Service
#apiVersion: v1
#metadata:
#  name: otel-demo-opensearch
#  labels:
#    app.kubernetes.io/name: opensearch
#    app.kubernetes.io/instance: opentelemetry-demo
#    app.kubernetes.io/version: "2.18.0"
#    app.kubernetes.io/component: otel-demo-opensearch
#  annotations: {}
#spec:
#  type: ClusterIP
#  selector:
#    app.kubernetes.io/name: opensearch
#    app.kubernetes.io/instance: opentelemetry-demo
#  ports:
#    - name: http
#      protocol: TCP
#      port: 9200
#    - name: transport
#      protocol: TCP
#      port: 9300
#    - name: metrics
#      protocol: TCP
#      port: 9600
#---
# Source: opentelemetry-demo/charts/opensearch/templates/service.yaml
#kind: Service
#apiVersion: v1
#metadata:
#  name: otel-demo-opensearch-headless
#  labels:
#    app.kubernetes.io/name: opensearch
#    app.kubernetes.io/instance: opentelemetry-demo
#    app.kubernetes.io/version: "2.18.0"
#    app.kubernetes.io/component: otel-demo-opensearch
#  annotations:
#    service.alpha.kubernetes.io/tolerate-unready-endpoints: "true"
#spec:
#  clusterIP: None # This is needed for statefulset hostnames like opensearch-0 to resolve
#  # Create endpoints also if the related pod isn't ready
#  publishNotReadyAddresses: true
#  selector:
#    app.kubernetes.io/name: opensearch
#    app.kubernetes.io/instance: opentelemetry-demo
#  ports:
#    - name: http
#      port: 9200
#    - name: transport
#      port: 9300
#    - name: metrics
#      port: 9600
#---
# Source: opentelemetry-demo/charts/opentelemetry-collector/templates/service.yaml
#apiVersion: v1
#kind: Service
#metadata:
#  name: opentelemetry-demo-otelcol
#  namespace: otel-demo
#  labels:
#    app.kubernetes.io/name: otelcol
#    app.kubernetes.io/instance: opentelemetry-demo
#    app.kubernetes.io/version: "0.113.0"
#
#    component: standalone-collector
#spec:
#  type: ClusterIP
#  ports:
#    - name: jaeger-compact
#      port: 6831
#      targetPort: 6831
#      protocol: UDP
#    - name: jaeger-grpc
#      port: 14250
#      targetPort: 14250
#      protocol: TCP
#    - name: jaeger-thrift
#      port: 14268
#      targetPort: 14268
#      protocol: TCP
#    - name: metrics
#      port: 8888
#      targetPort: 8888
#      protocol: TCP
#    - name: otlp
#      port: 4317
#      targetPort: 4317
#      protocol: TCP
#      appProtocol: grpc
#    - name: otlp-http
#      port: 4318
#      targetPort: 4318
#      protocol: TCP
#    - name: prometheus
#      port: 9464
#      targetPort: 9464
#      protocol: TCP
#    - name: zipkin
#      port: 9411
#      targetPort: 9411
#      protocol: TCP
#  selector:
#    app.kubernetes.io/name: otelcol
#    app.kubernetes.io/instance: opentelemetry-demo
#    component: standalone-collector
#  internalTrafficPolicy: Cluster
#---
# Source: opentelemetry-demo/charts/prometheus/templates/service.yaml
#apiVersion: v1
#kind: Service
#metadata:
#  labels:
#    app.kubernetes.io/component: server
#    app.kubernetes.io/name: prometheus
#    app.kubernetes.io/instance: opentelemetry-demo
#    app.kubernetes.io/version: v2.55.1
#    app.kubernetes.io/part-of: prometheus
#  name: opentelemetry-demo-prometheus-server
#  namespace: otel-demo
#spec:
#  ports:
#    - name: http
#      port: 9090
#      protocol: TCP
#      targetPort: 9090
#  selector:
#    app.kubernetes.io/component: server
#    app.kubernetes.io/name: prometheus
#    app.kubernetes.io/instance: opentelemetry-demo
#  sessionAffinity: None
#  type: "ClusterIP"
#---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: opentelemetry-demo-adservice
  labels:
    opentelemetry.io/name: opentelemetry-demo-adservice
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: adservice
    app.kubernetes.io/name: opentelemetry-demo-adservice
    app.kubernetes.io/version: "1.12.0"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  type: ClusterIP
  ports:
    - port: 8080
      name: tcp-service
      targetPort: 8080
  selector:
    opentelemetry.io/name: opentelemetry-demo-adservice
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: opentelemetry-demo-cartservice
  labels:
    opentelemetry.io/name: opentelemetry-demo-cartservice
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: cartservice
    app.kubernetes.io/name: opentelemetry-demo-cartservice
    app.kubernetes.io/version: "1.12.0"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  type: ClusterIP
  ports:
    - port: 8080
      name: tcp-service
      targetPort: 8080
  selector:
    opentelemetry.io/name: opentelemetry-demo-cartservice
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: opentelemetry-demo-checkoutservice
  labels:
    opentelemetry.io/name: opentelemetry-demo-checkoutservice
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: checkoutservice
    app.kubernetes.io/name: opentelemetry-demo-checkoutservice
    app.kubernetes.io/version: "1.12.0"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  type: ClusterIP
  ports:
    - port: 8080
      name: tcp-service
      targetPort: 8080
  selector:
    opentelemetry.io/name: opentelemetry-demo-checkoutservice
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: opentelemetry-demo-currencyservice
  labels:
    opentelemetry.io/name: opentelemetry-demo-currencyservice
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: currencyservice
    app.kubernetes.io/name: opentelemetry-demo-currencyservice
    app.kubernetes.io/version: "1.12.0"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  type: ClusterIP
  ports:
    - port: 8080
      name: tcp-service
      targetPort: 8080
  selector:
    opentelemetry.io/name: opentelemetry-demo-currencyservice
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: opentelemetry-demo-emailservice
  labels:
    opentelemetry.io/name: opentelemetry-demo-emailservice
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: emailservice
    app.kubernetes.io/name: opentelemetry-demo-emailservice
    app.kubernetes.io/version: "1.12.0"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  type: ClusterIP
  ports:
    - port: 8080
      name: tcp-service
      targetPort: 8080
  selector:
    opentelemetry.io/name: opentelemetry-demo-emailservice
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: opentelemetry-demo-flagd
  labels:
    opentelemetry.io/name: opentelemetry-demo-flagd
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: flagd
    app.kubernetes.io/name: opentelemetry-demo-flagd
    app.kubernetes.io/version: "1.12.0"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  type: ClusterIP
  ports:
    - port: 8013
      name: tcp-service
      targetPort: 8013
    - port: 4000
      name: tcp-service-0
      targetPort: 4000
  selector:
    opentelemetry.io/name: opentelemetry-demo-flagd
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: opentelemetry-demo-frontend
  labels:
    opentelemetry.io/name: opentelemetry-demo-frontend
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: frontend
    app.kubernetes.io/name: opentelemetry-demo-frontend
    app.kubernetes.io/version: "1.12.0"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  type: ClusterIP
  ports:
    - port: 8080
      name: tcp-service
      targetPort: 8080
  selector:
    opentelemetry.io/name: opentelemetry-demo-frontend
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: opentelemetry-demo-frontendproxy
  labels:
    opentelemetry.io/name: opentelemetry-demo-frontendproxy
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: frontendproxy
    app.kubernetes.io/name: opentelemetry-demo-frontendproxy
    app.kubernetes.io/version: "1.12.0"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  type: ClusterIP
  ports:
    - port: 8080
      name: tcp-service
      targetPort: 8080
  selector:
    opentelemetry.io/name: opentelemetry-demo-frontendproxy
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: opentelemetry-demo-imageprovider
  labels:
    opentelemetry.io/name: opentelemetry-demo-imageprovider
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: imageprovider
    app.kubernetes.io/name: opentelemetry-demo-imageprovider
    app.kubernetes.io/version: "1.12.0"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  type: ClusterIP
  ports:
    - port: 8081
      name: tcp-service
      targetPort: 8081
  selector:
    opentelemetry.io/name: opentelemetry-demo-imageprovider
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: opentelemetry-demo-kafka
  labels:
    opentelemetry.io/name: opentelemetry-demo-kafka
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: kafka
    app.kubernetes.io/name: opentelemetry-demo-kafka
    app.kubernetes.io/version: "1.12.0"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  type: ClusterIP
  ports:
    - port: 9092
      name: plaintext
      targetPort: 9092
    - port: 9093
      name: controller
      targetPort: 9093
  selector:
    opentelemetry.io/name: opentelemetry-demo-kafka
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: opentelemetry-demo-loadgenerator
  labels:
    opentelemetry.io/name: opentelemetry-demo-loadgenerator
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: loadgenerator
    app.kubernetes.io/name: opentelemetry-demo-loadgenerator
    app.kubernetes.io/version: "1.12.0"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  type: ClusterIP
  ports:
    - port: 8089
      name: tcp-service
      targetPort: 8089
  selector:
    opentelemetry.io/name: opentelemetry-demo-loadgenerator
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: opentelemetry-demo-paymentservice
  labels:
    opentelemetry.io/name: opentelemetry-demo-paymentservice
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: paymentservice
    app.kubernetes.io/name: opentelemetry-demo-paymentservice
    app.kubernetes.io/version: "1.12.0"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  type: ClusterIP
  ports:
    - port: 8080
      name: tcp-service
      targetPort: 8080
  selector:
    opentelemetry.io/name: opentelemetry-demo-paymentservice
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: opentelemetry-demo-productcatalogservice
  labels:
    opentelemetry.io/name: opentelemetry-demo-productcatalogservice
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: productcatalogservice
    app.kubernetes.io/name: opentelemetry-demo-productcatalogservice
    app.kubernetes.io/version: "1.12.0"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  type: ClusterIP
  ports:
    - port: 8080
      name: tcp-service
      targetPort: 8080
  selector:
    opentelemetry.io/name: opentelemetry-demo-productcatalogservice
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: opentelemetry-demo-quoteservice
  labels:
    opentelemetry.io/name: opentelemetry-demo-quoteservice
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: quoteservice
    app.kubernetes.io/name: opentelemetry-demo-quoteservice
    app.kubernetes.io/version: "1.12.0"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  type: ClusterIP
  ports:
    - port: 8080
      name: tcp-service
      targetPort: 8080
  selector:
    opentelemetry.io/name: opentelemetry-demo-quoteservice
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: opentelemetry-demo-recommendationservice
  labels:
    opentelemetry.io/name: opentelemetry-demo-recommendationservice
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: recommendationservice
    app.kubernetes.io/name: opentelemetry-demo-recommendationservice
    app.kubernetes.io/version: "1.12.0"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  type: ClusterIP
  ports:
    - port: 8080
      name: tcp-service
      targetPort: 8080
  selector:
    opentelemetry.io/name: opentelemetry-demo-recommendationservice
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: opentelemetry-demo-shippingservice
  labels:
    opentelemetry.io/name: opentelemetry-demo-shippingservice
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: shippingservice
    app.kubernetes.io/name: opentelemetry-demo-shippingservice
    app.kubernetes.io/version: "1.12.0"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  type: ClusterIP
  ports:
    - port: 8080
      name: tcp-service
      targetPort: 8080
  selector:
    opentelemetry.io/name: opentelemetry-demo-shippingservice
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: v1
kind: Service
metadata:
  name: opentelemetry-demo-valkey
  labels:
    opentelemetry.io/name: opentelemetry-demo-valkey
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: valkey
    app.kubernetes.io/name: opentelemetry-demo-valkey
    app.kubernetes.io/version: "1.12.0"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  type: ClusterIP
  ports:
    - port: 6379
      name: valkey
      targetPort: 6379
  selector:
    opentelemetry.io/name: opentelemetry-demo-valkey
---
# Source: opentelemetry-demo/charts/grafana/templates/deployment.yaml
#apiVersion: apps/v1
#kind: Deployment
#metadata:
#  name: opentelemetry-demo-grafana
#  namespace: otel-demo
#  labels:
#    app.kubernetes.io/name: grafana
#    app.kubernetes.io/instance: opentelemetry-demo
#    app.kubernetes.io/version: "11.3.0"
#spec:
#  replicas: 1
#  revisionHistoryLimit: 10
#  selector:
#    matchLabels:
#      app.kubernetes.io/name: grafana
#      app.kubernetes.io/instance: opentelemetry-demo
#  strategy:
#    type: RollingUpdate
#  template:
#    metadata:
#      labels:
#        app.kubernetes.io/name: grafana
#        app.kubernetes.io/instance: opentelemetry-demo
#      annotations:
#        checksum/config: 66402109ab73b3549330f38a66f20b78067ca4fdd1b77fc12fd760727d05f34c
#        checksum/sc-dashboard-provider-config: e70bf6a851099d385178a76de9757bb0bef8299da6d8443602590e44f05fdf24
#        checksum/secret: bed677784356b2af7fb0d87455db21f077853059b594101a4f6532bfbd962a7f
#        kubectl.kubernetes.io/default-container: grafana
#    spec:
#      serviceAccountName: opentelemetry-demo-grafana
#      automountServiceAccountToken: true
#      securityContext:
#        fsGroup: 472
#        runAsGroup: 472
#        runAsNonRoot: true
#        runAsUser: 472
#      enableServiceLinks: true
#      containers:
#        - name: grafana
#          image: "docker.io/grafana/grafana:11.3.0"
#          imagePullPolicy: IfNotPresent
#          securityContext:
#            allowPrivilegeEscalation: false
#            capabilities:
#              drop:
#                - ALL
#            seccompProfile:
#              type: RuntimeDefault
#          volumeMounts:
#            - name: config
#              mountPath: "/etc/grafana/grafana.ini"
#              subPath: grafana.ini
#            - name: storage
#              mountPath: "/var/lib/grafana"
#            - name: dashboards-default
#              mountPath: "/var/lib/grafana/dashboards/default"
#            - name: config
#              mountPath: "/etc/grafana/provisioning/datasources/datasources.yaml"
#              subPath: "datasources.yaml"
#            - name: config
#              mountPath: "/etc/grafana/provisioning/dashboards/dashboardproviders.yaml"
#              subPath: "dashboardproviders.yaml"
#          ports:
#            - name: grafana
#              containerPort: 3000
#              protocol: TCP
#            - name: gossip-tcp
#              containerPort: 9094
#              protocol: TCP
#            - name: gossip-udp
#              containerPort: 9094
#              protocol: UDP
#          env:
#            - name: POD_IP
#              valueFrom:
#                fieldRef:
#                  fieldPath: status.podIP
#            - name: GF_SECURITY_ADMIN_USER
#              valueFrom:
#                secretKeyRef:
#                  name: opentelemetry-demo-grafana
#                  key: admin-user
#            - name: GF_SECURITY_ADMIN_PASSWORD
#              valueFrom:
#                secretKeyRef:
#                  name: opentelemetry-demo-grafana
#                  key: admin-password
#            - name: GF_INSTALL_PLUGINS
#              valueFrom:
#                configMapKeyRef:
#                  name: opentelemetry-demo-grafana
#                  key: plugins
#            - name: GF_PATHS_DATA
#              value: /var/lib/grafana/
#            - name: GF_PATHS_LOGS
#              value: /var/log/grafana
#            - name: GF_PATHS_PLUGINS
#              value: /var/lib/grafana/plugins
#            - name: GF_PATHS_PROVISIONING
#              value: /etc/grafana/provisioning
#          livenessProbe:
#            failureThreshold: 10
#            httpGet:
#              path: /api/health
#              port: 3000
#            initialDelaySeconds: 60
#            timeoutSeconds: 30
#          readinessProbe:
#            httpGet:
#              path: /api/health
#              port: 3000
#          resources:
#            limits:
#              memory: 150Mi
#      volumes:
#        - name: config
#          configMap:
#            name: opentelemetry-demo-grafana
#        - name: dashboards-default
#          configMap:
#            name: opentelemetry-demo-grafana-dashboards
#        - name: storage
#          emptyDir: {}
#---
# Source: opentelemetry-demo/charts/jaeger/templates/allinone-deploy.yaml
#apiVersion: apps/v1
#kind: Deployment
#metadata:
#  name: opentelemetry-demo-jaeger
#  labels:
#    app.kubernetes.io/name: jaeger
#    app.kubernetes.io/instance: opentelemetry-demo
#    app.kubernetes.io/version: "1.53.0"
#    app.kubernetes.io/component: all-in-one
#    prometheus.io/port: "14269"
#    prometheus.io/scrape: "true"
#spec:
#  replicas: 1
#  strategy:
#    type: Recreate
#  selector:
#    matchLabels:
#      app.kubernetes.io/name: jaeger
#      app.kubernetes.io/instance: opentelemetry-demo
#      app.kubernetes.io/component: all-in-one
#  template:
#    metadata:
#      labels:
#        app.kubernetes.io/name: jaeger
#        app.kubernetes.io/instance: opentelemetry-demo
#        app.kubernetes.io/component: all-in-one
#      annotations:
#        prometheus.io/port: "14269"
#        prometheus.io/scrape: "true"
#    spec:
#      containers:
#        - env:
#            - name: METRICS_STORAGE_TYPE
#              value: prometheus
#            - name: COLLECTOR_OTLP_GRPC_HOST_PORT
#              value: 0.0.0.0:4317
#            - name: COLLECTOR_OTLP_HTTP_HOST_PORT
#              value: 0.0.0.0:4318
#            - name: SPAN_STORAGE_TYPE
#              value: memory
#
#            - name: COLLECTOR_ZIPKIN_HOST_PORT
#              value: :9411
#            - name: JAEGER_DISABLED
#              value: "false"
#            - name: COLLECTOR_OTLP_ENABLED
#              value: "true"
#          securityContext: {}
#          image: jaegertracing/all-in-one:1.53.0
#          imagePullPolicy: IfNotPresent
#          name: jaeger
#          args:
#            - "--memory.max-traces=5000"
#            - "--query.base-path=/jaeger/ui"
#            - "--prometheus.server-url=http://opentelemetry-demo-prometheus-server:9090"
#            - "--prometheus.query.normalize-calls=true"
#            - "--prometheus.query.normalize-duration=true"
#          ports:
#            - containerPort: 5775
#              protocol: UDP
#            - containerPort: 6831
#              protocol: UDP
#            - containerPort: 6832
#              protocol: UDP
#            - containerPort: 5778
#              protocol: TCP
#            - containerPort: 16686
#              protocol: TCP
#            - containerPort: 16685
#              protocol: TCP
#            - containerPort: 9411
#              protocol: TCP
#            - containerPort: 4317
#              protocol: TCP
#            - containerPort: 4318
#              protocol: TCP
#          livenessProbe:
#            failureThreshold: 5
#            httpGet:
#              path: /
#              port: 14269
#              scheme: HTTP
#            initialDelaySeconds: 5
#            periodSeconds: 15
#            successThreshold: 1
#            timeoutSeconds: 1
#          readinessProbe:
#            failureThreshold: 3
#            httpGet:
#              path: /
#              port: 14269
#              scheme: HTTP
#            initialDelaySeconds: 1
#            periodSeconds: 10
#            successThreshold: 1
#            timeoutSeconds: 1
#          resources:
#            limits:
#              memory: 400Mi
#          volumeMounts:
#      securityContext:
#        fsGroup: 10001
#        runAsGroup: 10001
#        runAsUser: 10001
#      serviceAccountName: opentelemetry-demo-jaeger
#      volumes:
#---
# Source: opentelemetry-demo/charts/opentelemetry-collector/templates/deployment.yaml
#apiVersion: apps/v1
#kind: Deployment
#metadata:
#  name: opentelemetry-demo-otelcol
#  namespace: otel-demo
#  labels:
#    app.kubernetes.io/name: otelcol
#    app.kubernetes.io/instance: opentelemetry-demo
#    app.kubernetes.io/version: "0.113.0"
#
#spec:
#  replicas: 1
#  revisionHistoryLimit: 10
#  selector:
#    matchLabels:
#      app.kubernetes.io/name: otelcol
#      app.kubernetes.io/instance: opentelemetry-demo
#      component: standalone-collector
#  strategy:
#    type: RollingUpdate
#  template:
#    metadata:
#      annotations:
#        checksum/config: 2d923ab8c3aa2c63ec32fe4a7aa4993b12f3e03b2e255f1909b97f5e43b58226
#        opentelemetry_community_demo: "true"
#        prometheus.io/port: "9464"
#        prometheus.io/scrape: "true"
#      labels:
#        app.kubernetes.io/name: otelcol
#        app.kubernetes.io/instance: opentelemetry-demo
#        component: standalone-collector
#
#    spec:
#      serviceAccountName: opentelemetry-demo-otelcol
#      securityContext: {}
#      containers:
#        - name: opentelemetry-collector
#          args:
#            - --config=/conf/relay.yaml
#          securityContext: {}
#          image: "otel/opentelemetry-collector-contrib:0.113.0"
#          imagePullPolicy: IfNotPresent
#          ports:
#            - name: jaeger-compact
#              containerPort: 6831
#              protocol: UDP
#            - name: jaeger-grpc
#              containerPort: 14250
#              protocol: TCP
#            - name: jaeger-thrift
#              containerPort: 14268
#              protocol: TCP
#            - name: metrics
#              containerPort: 8888
#              protocol: TCP
#            - name: otlp
#              containerPort: 4317
#              protocol: TCP
#            - name: otlp-http
#              containerPort: 4318
#              protocol: TCP
#            - name: prometheus
#              containerPort: 9464
#              protocol: TCP
#            - name: zipkin
#              containerPort: 9411
#              protocol: TCP
#          env:
#            - name: MY_POD_IP
#              valueFrom:
#                fieldRef:
#                  apiVersion: v1
#                  fieldPath: status.podIP
#            - name: GOMEMLIMIT
#              value: "160MiB"
#          livenessProbe:
#            httpGet:
#              path: /
#              port: 13133
#          readinessProbe:
#            httpGet:
#              path: /
#              port: 13133
#          resources:
#            limits:
#              memory: 200Mi
#          volumeMounts:
#            - mountPath: /conf
#              name: opentelemetry-collector-configmap
#      volumes:
#        - name: opentelemetry-collector-configmap
#          configMap:
#            name: opentelemetry-demo-otelcol
#            items:
#              - key: relay
#                path: relay.yaml
#      hostNetwork: false
#---
# Source: opentelemetry-demo/charts/prometheus/templates/deploy.yaml
#apiVersion: apps/v1
#kind: Deployment
#metadata:
#  labels:
#    app.kubernetes.io/component: server
#    app.kubernetes.io/name: prometheus
#    app.kubernetes.io/instance: opentelemetry-demo
#    app.kubernetes.io/version: v2.55.1
#    app.kubernetes.io/part-of: prometheus
#  name: opentelemetry-demo-prometheus-server
#  namespace: otel-demo
#spec:
#  selector:
#    matchLabels:
#      app.kubernetes.io/component: server
#      app.kubernetes.io/name: prometheus
#      app.kubernetes.io/instance: opentelemetry-demo
#  replicas: 1
#  revisionHistoryLimit: 10
#  strategy:
#    type: Recreate
#    rollingUpdate: null
#  template:
#    metadata:
#      labels:
#        app.kubernetes.io/component: server
#        app.kubernetes.io/name: prometheus
#        app.kubernetes.io/instance: opentelemetry-demo
#        app.kubernetes.io/version: v2.55.1
#        app.kubernetes.io/part-of: prometheus
#    spec:
#      enableServiceLinks: true
#      serviceAccountName: opentelemetry-demo-prometheus-server
#      containers:
#        - name: prometheus-server
#          image: "quay.io/prometheus/prometheus:v2.55.1"
#          imagePullPolicy: "IfNotPresent"
#          args:
#            - --storage.tsdb.retention.time=15d
#            - --config.file=/etc/config/prometheus.yml
#            - --storage.tsdb.path=/data
#            - --web.console.libraries=/etc/prometheus/console_libraries
#            - --web.console.templates=/etc/prometheus/consoles
#            - --enable-feature=exemplar-storage
#            - --enable-feature=otlp-write-receiver
#          ports:
#            - containerPort: 9090
#          readinessProbe:
#            httpGet:
#              path: /-/ready
#              port: 9090
#              scheme: HTTP
#            initialDelaySeconds: 30
#            periodSeconds: 5
#            timeoutSeconds: 4
#            failureThreshold: 3
#            successThreshold: 1
#          livenessProbe:
#            httpGet:
#              path: /-/healthy
#              port: 9090
#              scheme: HTTP
#            initialDelaySeconds: 30
#            periodSeconds: 15
#            timeoutSeconds: 10
#            failureThreshold: 3
#            successThreshold: 1
#          resources:
#            limits:
#              memory: 300Mi
#          volumeMounts:
#            - name: config-volume
#              mountPath: /etc/config
#            - name: storage-volume
#              mountPath: /data
#              subPath: ""
#      dnsPolicy: ClusterFirst
#      securityContext:
#        fsGroup: 65534
#        runAsGroup: 65534
#        runAsNonRoot: true
#        runAsUser: 65534
#      terminationGracePeriodSeconds: 300
#      volumes:
#        - name: config-volume
#          configMap:
#            name: opentelemetry-demo-prometheus-server
#        - name: storage-volume
#          emptyDir: {}
#---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: opentelemetry-demo-accountingservice
  labels:
    opentelemetry.io/name: opentelemetry-demo-accountingservice
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: accountingservice
    app.kubernetes.io/name: opentelemetry-demo-accountingservice
    app.kubernetes.io/version: "1.12.0"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      opentelemetry.io/name: opentelemetry-demo-accountingservice
  template:
    metadata:
      labels:
        opentelemetry.io/name: opentelemetry-demo-accountingservice
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: accountingservice
        app.kubernetes.io/name: opentelemetry-demo-accountingservice
    spec:
      serviceAccountName: opentelemetry-demo
      containers:
        - name: accountingservice
          image: "ghcr.io/open-telemetry/demo:1.12.0-accountingservice"
          imagePullPolicy: IfNotPresent
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: "datadog.datadog.svc.cluster.local"
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: KAFKA_SERVICE_ADDR
              value: "opentelemetry-demo-kafka:9092"
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4317
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=1.12.0
          resources:
            limits:
              memory: 120Mi
          volumeMounts:
      initContainers:
        - command:
            - sh
            - -c
            - until nc -z -v -w30 opentelemetry-demo-kafka 9092; do echo waiting
              for kafka; sleep 2; done;
          image: busybox:latest
          name: wait-for-kafka
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: opentelemetry-demo-adservice
  labels:
    opentelemetry.io/name: opentelemetry-demo-adservice
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: adservice
    app.kubernetes.io/name: opentelemetry-demo-adservice
    app.kubernetes.io/version: "1.12.0"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      opentelemetry.io/name: opentelemetry-demo-adservice
  template:
    metadata:
      labels:
        opentelemetry.io/name: opentelemetry-demo-adservice
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: adservice
        app.kubernetes.io/name: opentelemetry-demo-adservice
    spec:
      serviceAccountName: opentelemetry-demo
      containers:
        - name: adservice
          image: "ghcr.io/open-telemetry/demo:1.12.0-adservice"
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8080
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: "datadog.datadog.svc.cluster.local"
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: AD_SERVICE_PORT
              value: "8080"
            - name: FLAGD_HOST
              value: "opentelemetry-demo-flagd"
            - name: FLAGD_PORT
              value: "8013"
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4318
            - name: OTEL_LOGS_EXPORTER
              value: otlp
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=1.12.0
          resources:
            limits:
              memory: 300Mi
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: opentelemetry-demo-cartservice
  labels:
    opentelemetry.io/name: opentelemetry-demo-cartservice
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: cartservice
    app.kubernetes.io/name: opentelemetry-demo-cartservice
    app.kubernetes.io/version: "1.12.0"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      opentelemetry.io/name: opentelemetry-demo-cartservice
  template:
    metadata:
      labels:
        opentelemetry.io/name: opentelemetry-demo-cartservice
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: cartservice
        app.kubernetes.io/name: opentelemetry-demo-cartservice
    spec:
      serviceAccountName: opentelemetry-demo
      containers:
        - name: cartservice
          image: "ghcr.io/open-telemetry/demo:1.12.0-cartservice"
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8080
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: "datadog.datadog.svc.cluster.local"
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: CART_SERVICE_PORT
              value: "8080"
            - name: ASPNETCORE_URLS
              value: http://*:$(CART_SERVICE_PORT)
            - name: VALKEY_ADDR
              value: "opentelemetry-demo-valkey:6379"
            - name: FLAGD_HOST
              value: "opentelemetry-demo-flagd"
            - name: FLAGD_PORT
              value: "8013"
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4317
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=1.12.0
          resources:
            limits:
              memory: 160Mi
          volumeMounts:
      initContainers:
        - command:
            - sh
            - -c
            - until nc -z -v -w30 opentelemetry-demo-valkey 6379; do echo waiting
              for valkey; sleep 2; done;
          image: busybox:latest
          name: wait-for-valkey
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: opentelemetry-demo-checkoutservice
  labels:
    opentelemetry.io/name: opentelemetry-demo-checkoutservice
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: checkoutservice
    app.kubernetes.io/name: opentelemetry-demo-checkoutservice
    app.kubernetes.io/version: "1.12.0"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      opentelemetry.io/name: opentelemetry-demo-checkoutservice
  template:
    metadata:
      labels:
        opentelemetry.io/name: opentelemetry-demo-checkoutservice
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: checkoutservice
        app.kubernetes.io/name: opentelemetry-demo-checkoutservice
    spec:
      serviceAccountName: opentelemetry-demo
      containers:
        - name: checkoutservice
          image: "ghcr.io/open-telemetry/demo:1.12.0-checkoutservice"
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8080
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: "datadog.datadog.svc.cluster.local"
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: CHECKOUT_SERVICE_PORT
              value: "8080"
            - name: CART_SERVICE_ADDR
              value: "opentelemetry-demo-cartservice:8080"
            - name: CURRENCY_SERVICE_ADDR
              value: "opentelemetry-demo-currencyservice:8080"
            - name: EMAIL_SERVICE_ADDR
              value: http://opentelemetry-demo-emailservice:8080
            - name: PAYMENT_SERVICE_ADDR
              value: "opentelemetry-demo-paymentservice:8080"
            - name: PRODUCT_CATALOG_SERVICE_ADDR
              value: "opentelemetry-demo-productcatalogservice:8080"
            - name: SHIPPING_SERVICE_ADDR
              value: "opentelemetry-demo-shippingservice:8080"
            - name: KAFKA_SERVICE_ADDR
              value: "opentelemetry-demo-kafka:9092"
            - name: FLAGD_HOST
              value: "opentelemetry-demo-flagd"
            - name: FLAGD_PORT
              value: "8013"
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4317
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=1.12.0
          resources:
            limits:
              memory: 20Mi
          volumeMounts:
      initContainers:
        - command:
            - sh
            - -c
            - until nc -z -v -w30 opentelemetry-demo-kafka 9092; do echo waiting
              for kafka; sleep 2; done;
          image: busybox:latest
          name: wait-for-kafka
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: opentelemetry-demo-currencyservice
  labels:
    opentelemetry.io/name: opentelemetry-demo-currencyservice
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: currencyservice
    app.kubernetes.io/name: opentelemetry-demo-currencyservice
    app.kubernetes.io/version: "1.12.0"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      opentelemetry.io/name: opentelemetry-demo-currencyservice
  template:
    metadata:
      labels:
        opentelemetry.io/name: opentelemetry-demo-currencyservice
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: currencyservice
        app.kubernetes.io/name: opentelemetry-demo-currencyservice
    spec:
      serviceAccountName: opentelemetry-demo
      containers:
        - name: currencyservice
          image: "ghcr.io/open-telemetry/demo:1.12.0-currencyservice"
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8080
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: "datadog.datadog.svc.cluster.local"
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: CURRENCY_SERVICE_PORT
              value: "8080"
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4317
            - name: VERSION
              value: "1.12.0"
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=1.12.0
          resources:
            limits:
              memory: 20Mi
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: opentelemetry-demo-emailservice
  labels:
    opentelemetry.io/name: opentelemetry-demo-emailservice
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: emailservice
    app.kubernetes.io/name: opentelemetry-demo-emailservice
    app.kubernetes.io/version: "1.12.0"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      opentelemetry.io/name: opentelemetry-demo-emailservice
  template:
    metadata:
      labels:
        opentelemetry.io/name: opentelemetry-demo-emailservice
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: emailservice
        app.kubernetes.io/name: opentelemetry-demo-emailservice
    spec:
      serviceAccountName: opentelemetry-demo
      containers:
        - name: emailservice
          image: "ghcr.io/open-telemetry/demo:1.12.0-emailservice"
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8080
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: "datadog.datadog.svc.cluster.local"
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: EMAIL_SERVICE_PORT
              value: "8080"
            - name: APP_ENV
              value: production
            - name: OTEL_EXPORTER_OTLP_TRACES_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4318/v1/traces
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=1.12.0
          resources:
            limits:
              memory: 100Mi
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: opentelemetry-demo-flagd
  labels:
    opentelemetry.io/name: opentelemetry-demo-flagd
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: flagd
    app.kubernetes.io/name: opentelemetry-demo-flagd
    app.kubernetes.io/version: "1.12.0"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      opentelemetry.io/name: opentelemetry-demo-flagd
  template:
    metadata:
      labels:
        opentelemetry.io/name: opentelemetry-demo-flagd
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: flagd
        app.kubernetes.io/name: opentelemetry-demo-flagd
    spec:
      serviceAccountName: opentelemetry-demo
      containers:
        - name: flagd
          image: "ghcr.io/open-feature/flagd:v0.11.1"
          imagePullPolicy: IfNotPresent
          command:
            - /flagd-build
            - start
            - --uri
            - file:./etc/flagd/demo.flagd.json
          ports:
            - containerPort: 8013
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: "datadog.datadog.svc.cluster.local"
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: FLAGD_METRICS_EXPORTER
              value: otel
            - name: FLAGD_OTEL_COLLECTOR_URI
              value: $(OTEL_COLLECTOR_NAME):4317
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=1.12.0
          resources:
            limits:
              memory: 50Mi
          volumeMounts:
            - name: config-rw
              mountPath: /etc/flagd
        - name: flagdui
          image: "ghcr.io/open-telemetry/demo:1.12.0-flagdui"
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 4000
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: "opentelemetry-demo-otelcol"
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: FLAGD_METRICS_EXPORTER
              value: otel
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4317
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=1.12.0
          resources:
            limits:
              memory: 150Mi
          volumeMounts:
            - mountPath: /app/data
              name: config-rw
      initContainers:
        - command:
            - sh
            - -c
            - cp /config-ro/demo.flagd.json /config-rw/demo.flagd.json && cat /config-rw/demo.flagd.json
          image: busybox
          name: init-config
          volumeMounts:
            - mountPath: /config-ro
              name: config-ro
            - mountPath: /config-rw
              name: config-rw
      volumes:
        - name: config-rw
          emptyDir: {}
        - configMap:
            name: "opentelemetry-demo-flagd-config"
          name: config-ro
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: opentelemetry-demo-frauddetectionservice
  labels:
    opentelemetry.io/name: opentelemetry-demo-frauddetectionservice
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: frauddetectionservice
    app.kubernetes.io/name: opentelemetry-demo-frauddetectionservice
    app.kubernetes.io/version: "1.12.0"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      opentelemetry.io/name: opentelemetry-demo-frauddetectionservice
  template:
    metadata:
      labels:
        opentelemetry.io/name: opentelemetry-demo-frauddetectionservice
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: frauddetectionservice
        app.kubernetes.io/name: opentelemetry-demo-frauddetectionservice
    spec:
      serviceAccountName: opentelemetry-demo
      containers:
        - name: frauddetectionservice
          image: "ghcr.io/open-telemetry/demo:1.12.0-frauddetectionservice"
          imagePullPolicy: IfNotPresent
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: "datadog.datadog.svc.cluster.local"
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: KAFKA_SERVICE_ADDR
              value: "opentelemetry-demo-kafka:9092"
            - name: FLAGD_HOST
              value: "opentelemetry-demo-flagd"
            - name: FLAGD_PORT
              value: "8013"
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4318
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=1.12.0
          resources:
            limits:
              memory: 300Mi
          volumeMounts:
      initContainers:
        - command:
            - sh
            - -c
            - until nc -z -v -w30 opentelemetry-demo-kafka 9092; do echo waiting
              for kafka; sleep 2; done;
          image: busybox:latest
          name: wait-for-kafka
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: opentelemetry-demo-frontend
  labels:
    opentelemetry.io/name: opentelemetry-demo-frontend
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: frontend
    app.kubernetes.io/name: opentelemetry-demo-frontend
    app.kubernetes.io/version: "1.12.0"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      opentelemetry.io/name: opentelemetry-demo-frontend
  template:
    metadata:
      labels:
        opentelemetry.io/name: opentelemetry-demo-frontend
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: frontend
        app.kubernetes.io/name: opentelemetry-demo-frontend
    spec:
      serviceAccountName: opentelemetry-demo
      containers:
        - name: frontend
          image: "ghcr.io/open-telemetry/demo:1.12.0-frontend"
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8080
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: "datadog.datadog.svc.cluster.local"
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: FRONTEND_PORT
              value: "8080"
            - name: FRONTEND_ADDR
              value: :8080
            - name: AD_SERVICE_ADDR
              value: "opentelemetry-demo-adservice:8080"
            - name: CART_SERVICE_ADDR
              value: "opentelemetry-demo-cartservice:8080"
            - name: CHECKOUT_SERVICE_ADDR
              value: "opentelemetry-demo-checkoutservice:8080"
            - name: CURRENCY_SERVICE_ADDR
              value: "opentelemetry-demo-currencyservice:8080"
            - name: PRODUCT_CATALOG_SERVICE_ADDR
              value: "opentelemetry-demo-productcatalogservice:8080"
            - name: RECOMMENDATION_SERVICE_ADDR
              value: "opentelemetry-demo-recommendationservice:8080"
            - name: SHIPPING_SERVICE_ADDR
              value: "opentelemetry-demo-shippingservice:8080"
            - name: FLAGD_HOST
              value: "opentelemetry-demo-flagd"
            - name: FLAGD_PORT
              value: "8013"
            - name: OTEL_COLLECTOR_HOST
              value: $(OTEL_COLLECTOR_NAME)
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4317
            - name: WEB_OTEL_SERVICE_NAME
              value: frontend-web
            - name: PUBLIC_OTEL_EXPORTER_OTLP_TRACES_ENDPOINT
              value: http://localhost:8080/otlp-http/v1/traces
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=1.12.0
          resources:
            limits:
              memory: 250Mi
          securityContext:
            runAsGroup: 1001
            runAsNonRoot: true
            runAsUser: 1001
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: opentelemetry-demo-frontendproxy
  labels:
    opentelemetry.io/name: opentelemetry-demo-frontendproxy
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: frontendproxy
    app.kubernetes.io/name: opentelemetry-demo-frontendproxy
    app.kubernetes.io/version: "1.12.0"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      opentelemetry.io/name: opentelemetry-demo-frontendproxy
  template:
    metadata:
      labels:
        opentelemetry.io/name: opentelemetry-demo-frontendproxy
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: frontendproxy
        app.kubernetes.io/name: opentelemetry-demo-frontendproxy
    spec:
      serviceAccountName: opentelemetry-demo
      containers:
        - name: frontendproxy
          image: "ghcr.io/open-telemetry/demo:1.12.0-frontendproxy"
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8080
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: "datadog.datadog.svc.cluster.local"
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: ENVOY_PORT
              value: "8080"
            - name: FLAGD_HOST
              value: "opentelemetry-demo-flagd"
            - name: FLAGD_PORT
              value: "8013"
            - name: FLAGD_UI_HOST
              value: "opentelemetry-demo-flagd"
            - name: FLAGD_UI_PORT
              value: "4000"
            - name: FRONTEND_HOST
              value: "opentelemetry-demo-frontend"
            - name: FRONTEND_PORT
              value: "8080"
            - name: GRAFANA_SERVICE_HOST
              value: "opentelemetry-demo-grafana"
            - name: GRAFANA_SERVICE_PORT
              value: "80"
            - name: IMAGE_PROVIDER_HOST
              value: "opentelemetry-demo-imageprovider"
            - name: IMAGE_PROVIDER_PORT
              value: "8081"
            - name: JAEGER_SERVICE_HOST
              value: "opentelemetry-demo-jaeger-query"
            - name: JAEGER_SERVICE_PORT
              value: "16686"
            - name: LOCUST_WEB_HOST
              value: "opentelemetry-demo-loadgenerator"
            - name: LOCUST_WEB_PORT
              value: "8089"
            - name: OTEL_COLLECTOR_HOST
              value: $(OTEL_COLLECTOR_NAME)
            - name: OTEL_COLLECTOR_PORT_GRPC
              value: "4317"
            - name: OTEL_COLLECTOR_PORT_HTTP
              value: "4318"
            - name: OTEL_SERVICE_NAME
              value: frontend-proxy
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=1.12.0
          resources:
            limits:
              memory: 50Mi
          securityContext:
            runAsGroup: 101
            runAsNonRoot: true
            runAsUser: 101
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: opentelemetry-demo-imageprovider
  labels:
    opentelemetry.io/name: opentelemetry-demo-imageprovider
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: imageprovider
    app.kubernetes.io/name: opentelemetry-demo-imageprovider
    app.kubernetes.io/version: "1.12.0"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      opentelemetry.io/name: opentelemetry-demo-imageprovider
  template:
    metadata:
      labels:
        opentelemetry.io/name: opentelemetry-demo-imageprovider
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: imageprovider
        app.kubernetes.io/name: opentelemetry-demo-imageprovider
    spec:
      serviceAccountName: opentelemetry-demo
      containers:
        - name: imageprovider
          image: "ghcr.io/open-telemetry/demo:1.12.0-imageprovider"
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8081
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: "datadog.datadog.svc.cluster.local"
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: IMAGE_PROVIDER_PORT
              value: "8081"
            - name: OTEL_COLLECTOR_PORT_GRPC
              value: "4317"
            - name: OTEL_COLLECTOR_HOST
              value: $(OTEL_COLLECTOR_NAME)
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=1.12.0
          resources:
            limits:
              memory: 50Mi
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: opentelemetry-demo-kafka
  labels:
    opentelemetry.io/name: opentelemetry-demo-kafka
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: kafka
    app.kubernetes.io/name: opentelemetry-demo-kafka
    app.kubernetes.io/version: "1.12.0"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      opentelemetry.io/name: opentelemetry-demo-kafka
  template:
    metadata:
      labels:
        opentelemetry.io/name: opentelemetry-demo-kafka
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: kafka
        app.kubernetes.io/name: opentelemetry-demo-kafka
    spec:
      serviceAccountName: opentelemetry-demo
      containers:
        - name: kafka
          image: "ghcr.io/open-telemetry/demo:1.12.0-kafka"
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 9092
              name: plaintext
            - containerPort: 9093
              name: controller
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: "datadog.datadog.svc.cluster.local"
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: KAFKA_ADVERTISED_LISTENERS
              value: PLAINTEXT://opentelemetry-demo-kafka:9092
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4318
            - name: KAFKA_HEAP_OPTS
              value: -Xmx400M -Xms400M
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=1.12.0
          resources:
            limits:
              memory: 600Mi
          securityContext:
            runAsGroup: 1000
            runAsNonRoot: true
            runAsUser: 1000
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: opentelemetry-demo-loadgenerator
  labels:
    opentelemetry.io/name: opentelemetry-demo-loadgenerator
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: loadgenerator
    app.kubernetes.io/name: opentelemetry-demo-loadgenerator
    app.kubernetes.io/version: "1.12.0"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      opentelemetry.io/name: opentelemetry-demo-loadgenerator
  template:
    metadata:
      labels:
        opentelemetry.io/name: opentelemetry-demo-loadgenerator
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: loadgenerator
        app.kubernetes.io/name: opentelemetry-demo-loadgenerator
    spec:
      serviceAccountName: opentelemetry-demo
      containers:
        - name: loadgenerator
          image: "ghcr.io/open-telemetry/demo:1.12.0-loadgenerator"
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8089
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: "datadog.datadog.svc.cluster.local"
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: LOCUST_WEB_PORT
              value: "8089"
            - name: LOCUST_USERS
              value: "10"
            - name: LOCUST_SPAWN_RATE
              value: "1"
            - name: LOCUST_HOST
              value: http://opentelemetry-demo-frontendproxy:8080
            - name: LOCUST_HEADLESS
              value: "false"
            - name: LOCUST_AUTOSTART
              value: "true"
            - name: LOCUST_BROWSER_TRAFFIC_ENABLED
              value: "true"
            - name: PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION
              value: python
            - name: FLAGD_HOST
              value: "opentelemetry-demo-flagd"
            - name: FLAGD_PORT
              value: "8013"
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4317
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=1.12.0
          resources:
            limits:
              memory: 1Gi
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: opentelemetry-demo-paymentservice
  labels:
    opentelemetry.io/name: opentelemetry-demo-paymentservice
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: paymentservice
    app.kubernetes.io/name: opentelemetry-demo-paymentservice
    app.kubernetes.io/version: "1.12.0"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      opentelemetry.io/name: opentelemetry-demo-paymentservice
  template:
    metadata:
      labels:
        opentelemetry.io/name: opentelemetry-demo-paymentservice
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: paymentservice
        app.kubernetes.io/name: opentelemetry-demo-paymentservice
    spec:
      serviceAccountName: opentelemetry-demo
      containers:
        - name: paymentservice
          image: "ghcr.io/open-telemetry/demo:1.12.0-paymentservice"
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8080
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: "datadog.datadog.svc.cluster.local"
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: PAYMENT_SERVICE_PORT
              value: "8080"
            - name: FLAGD_HOST
              value: "opentelemetry-demo-flagd"
            - name: FLAGD_PORT
              value: "8013"
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4317
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=1.12.0
          resources:
            limits:
              memory: 120Mi
          securityContext:
            runAsGroup: 1000
            runAsNonRoot: true
            runAsUser: 1000
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: opentelemetry-demo-productcatalogservice
  labels:
    opentelemetry.io/name: opentelemetry-demo-productcatalogservice
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: productcatalogservice
    app.kubernetes.io/name: opentelemetry-demo-productcatalogservice
    app.kubernetes.io/version: "1.12.0"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      opentelemetry.io/name: opentelemetry-demo-productcatalogservice
  template:
    metadata:
      labels:
        opentelemetry.io/name: opentelemetry-demo-productcatalogservice
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: productcatalogservice
        app.kubernetes.io/name: opentelemetry-demo-productcatalogservice
    spec:
      serviceAccountName: opentelemetry-demo
      containers:
        - name: productcatalogservice
          image: "ghcr.io/open-telemetry/demo:1.12.0-productcatalogservice"
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8080
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: "datadog.datadog.svc.cluster.local"
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: PRODUCT_CATALOG_SERVICE_PORT
              value: "8080"
            - name: FLAGD_HOST
              value: "opentelemetry-demo-flagd"
            - name: FLAGD_PORT
              value: "8013"
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4317
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=1.12.0
          resources:
            limits:
              memory: 20Mi
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: opentelemetry-demo-quoteservice
  labels:
    opentelemetry.io/name: opentelemetry-demo-quoteservice
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: quoteservice
    app.kubernetes.io/name: opentelemetry-demo-quoteservice
    app.kubernetes.io/version: "1.12.0"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      opentelemetry.io/name: opentelemetry-demo-quoteservice
  template:
    metadata:
      labels:
        opentelemetry.io/name: opentelemetry-demo-quoteservice
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: quoteservice
        app.kubernetes.io/name: opentelemetry-demo-quoteservice
    spec:
      serviceAccountName: opentelemetry-demo
      containers:
        - name: quoteservice
          image: "ghcr.io/open-telemetry/demo:1.12.0-quoteservice"
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8080
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: "datadog.datadog.svc.cluster.local"
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: QUOTE_SERVICE_PORT
              value: "8080"
            - name: OTEL_PHP_AUTOLOAD_ENABLED
              value: "true"
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4318
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=1.12.0
          resources:
            limits:
              memory: 40Mi
          securityContext:
            runAsGroup: 33
            runAsNonRoot: true
            runAsUser: 33
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: opentelemetry-demo-recommendationservice
  labels:
    opentelemetry.io/name: opentelemetry-demo-recommendationservice
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: recommendationservice
    app.kubernetes.io/name: opentelemetry-demo-recommendationservice
    app.kubernetes.io/version: "1.12.0"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      opentelemetry.io/name: opentelemetry-demo-recommendationservice
  template:
    metadata:
      labels:
        opentelemetry.io/name: opentelemetry-demo-recommendationservice
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: recommendationservice
        app.kubernetes.io/name: opentelemetry-demo-recommendationservice
    spec:
      serviceAccountName: opentelemetry-demo
      containers:
        - name: recommendationservice
          image: "ghcr.io/open-telemetry/demo:1.12.0-recommendationservice"
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8080
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: "datadog.datadog.svc.cluster.local"
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: RECOMMENDATION_SERVICE_PORT
              value: "8080"
            - name: PRODUCT_CATALOG_SERVICE_ADDR
              value: "opentelemetry-demo-productcatalogservice:8080"
            - name: OTEL_PYTHON_LOG_CORRELATION
              value: "true"
            - name: PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION
              value: python
            - name: FLAGD_HOST
              value: "opentelemetry-demo-flagd"
            - name: FLAGD_PORT
              value: "8013"
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4317
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=1.12.0
          resources:
            limits:
              memory: 500Mi
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: opentelemetry-demo-shippingservice
  labels:
    opentelemetry.io/name: opentelemetry-demo-shippingservice
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: shippingservice
    app.kubernetes.io/name: opentelemetry-demo-shippingservice
    app.kubernetes.io/version: "1.12.0"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      opentelemetry.io/name: opentelemetry-demo-shippingservice
  template:
    metadata:
      labels:
        opentelemetry.io/name: opentelemetry-demo-shippingservice
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: shippingservice
        app.kubernetes.io/name: opentelemetry-demo-shippingservice
    spec:
      serviceAccountName: opentelemetry-demo
      containers:
        - name: shippingservice
          image: "ghcr.io/open-telemetry/demo:1.12.0-shippingservice"
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 8080
              name: service
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: "datadog.datadog.svc.cluster.local"
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: SHIPPING_SERVICE_PORT
              value: "8080"
            - name: QUOTE_SERVICE_ADDR
              value: http://opentelemetry-demo-quoteservice:8080
            - name: OTEL_EXPORTER_OTLP_ENDPOINT
              value: http://$(OTEL_COLLECTOR_NAME):4317
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=1.12.0
          resources:
            limits:
              memory: 20Mi
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/templates/component.yaml
apiVersion: apps/v1
kind: Deployment
metadata:
  name: opentelemetry-demo-valkey
  labels:
    opentelemetry.io/name: opentelemetry-demo-valkey
    app.kubernetes.io/instance: opentelemetry-demo
    app.kubernetes.io/component: valkey
    app.kubernetes.io/name: opentelemetry-demo-valkey
    app.kubernetes.io/version: "1.12.0"
    app.kubernetes.io/part-of: opentelemetry-demo
spec:
  replicas: 1
  revisionHistoryLimit: 10
  selector:
    matchLabels:
      opentelemetry.io/name: opentelemetry-demo-valkey
  template:
    metadata:
      labels:
        opentelemetry.io/name: opentelemetry-demo-valkey
        app.kubernetes.io/instance: opentelemetry-demo
        app.kubernetes.io/component: valkey
        app.kubernetes.io/name: opentelemetry-demo-valkey
    spec:
      serviceAccountName: opentelemetry-demo
      containers:
        - name: valkey
          image: "valkey/valkey:7.2-alpine"
          imagePullPolicy: IfNotPresent
          ports:
            - containerPort: 6379
              name: valkey
          env:
            - name: OTEL_SERVICE_NAME
              valueFrom:
                fieldRef:
                  apiVersion: v1
                  fieldPath: metadata.labels['app.kubernetes.io/component']
            - name: OTEL_COLLECTOR_NAME
              value: "datadog.datadog.svc.cluster.local"
            - name: OTEL_EXPORTER_OTLP_METRICS_TEMPORALITY_PREFERENCE
              value: cumulative
            - name: OTEL_RESOURCE_ATTRIBUTES
              value: service.name=$(OTEL_SERVICE_NAME),service.namespace=opentelemetry-demo,service.version=1.12.0
          resources:
            limits:
              memory: 20Mi
          securityContext:
            runAsGroup: 1000
            runAsNonRoot: true
            runAsUser: 999
          volumeMounts:
      volumes:
---
# Source: opentelemetry-demo/charts/opensearch/templates/statefulset.yaml
#apiVersion: apps/v1
#kind: StatefulSet
#metadata:
#  name: otel-demo-opensearch
#  labels:
#    app.kubernetes.io/name: opensearch
#    app.kubernetes.io/instance: opentelemetry-demo
#    app.kubernetes.io/version: "2.18.0"
#    app.kubernetes.io/component: otel-demo-opensearch
#  annotations:
#    majorVersion: "2"
#spec:
#  serviceName: otel-demo-opensearch-headless
#  selector:
#    matchLabels:
#      app.kubernetes.io/name: opensearch
#      app.kubernetes.io/instance: opentelemetry-demo
#  replicas: 1
#  podManagementPolicy: Parallel
#  updateStrategy:
#    type: RollingUpdate
#  template:
#    metadata:
#      name: "otel-demo-opensearch"
#      labels:
#        app.kubernetes.io/name: opensearch
#        app.kubernetes.io/instance: opentelemetry-demo
#        app.kubernetes.io/version: "2.18.0"
#        app.kubernetes.io/component: otel-demo-opensearch
#      annotations:
#        configchecksum: e8c450687c20323f6f710672deb70783c1872772b09c7e40d93ae72e602bec3
#    spec:
#      securityContext:
#        fsGroup: 1000
#        runAsUser: 1000
#      automountServiceAccountToken: false
#      affinity:
#        podAntiAffinity:
#          preferredDuringSchedulingIgnoredDuringExecution:
#            - weight: 1
#              podAffinityTerm:
#                topologyKey: kubernetes.io/hostname
#                labelSelector:
#                  matchExpressions:
#                    - key: app.kubernetes.io/instance
#                      operator: In
#                      values:
#                        - opentelemetry-demo
#                    - key: app.kubernetes.io/name
#                      operator: In
#                      values:
#                        - opensearch
#      terminationGracePeriodSeconds: 120
#      volumes:
#        - name: config
#          configMap:
#            name: otel-demo-opensearch-config
#        - emptyDir: {}
#          name: config-emptydir
#      enableServiceLinks: true
#      initContainers:
#        - name: configfile
#          image: "opensearchproject/opensearch:2.18.0"
#          imagePullPolicy: "IfNotPresent"
#          command:
#            - sh
#            - -c
#            - |
#              #!/usr/bin/env bash
#              cp -r /tmp/configfolder/*  /tmp/config/
#          resources: {}
#          volumeMounts:
#            - mountPath: /tmp/config/
#              name: config-emptydir
#            - name: config
#              mountPath: /tmp/configfolder/opensearch.yml
#              subPath: opensearch.yml
#      containers:
#        - name: "opensearch"
#          securityContext:
#            capabilities:
#              drop:
#                - ALL
#            runAsNonRoot: true
#            runAsUser: 1000
#
#          image: "opensearchproject/opensearch:2.18.0"
#          imagePullPolicy: "IfNotPresent"
#          readinessProbe:
#            failureThreshold: 3
#            periodSeconds: 5
#            tcpSocket:
#              port: 9200
#            timeoutSeconds: 3
#          startupProbe:
#            failureThreshold: 30
#            initialDelaySeconds: 5
#            periodSeconds: 10
#            tcpSocket:
#              port: 9200
#            timeoutSeconds: 3
#          ports:
#            - name: http
#              containerPort: 9200
#            - name: transport
#              containerPort: 9300
#            - name: metrics
#              containerPort: 9600
#          resources:
#            limits:
#              memory: 1Gi
#            requests:
#              cpu: 1000m
#              memory: 100Mi
#          env:
#            - name: node.name
#              valueFrom:
#                fieldRef:
#                  fieldPath: metadata.name
#            - name: discovery.seed_hosts
#              value: "opensearch-cluster-master-headless"
#            - name: cluster.name
#              value: "demo-cluster"
#            - name: network.host
#              value: "0.0.0.0"
#            - name: OPENSEARCH_JAVA_OPTS
#              value: "-Xms300m -Xmx300m"
#            - name: node.roles
#              value: "master,ingest,data,remote_cluster_client,"
#            - name: discovery.type
#              value: "single-node"
#            - name: bootstrap.memory_lock
#              value: "true"
#            - name: DISABLE_INSTALL_DEMO_CONFIG
#              value: "true"
#            - name: DISABLE_SECURITY_PLUGIN
#              value: "true"
#          volumeMounts:
#            - name: config-emptydir
#              mountPath: /usr/share/opensearch/config/opensearch.yml
#              subPath: opensearch.yml
